{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPT 423/820: Assignment 3  Question 6 \n",
    "* Solutions and Marking Guide\n",
    "\n",
    "In this Notebook, you'll use Python to explore Structure Learning in Bayesian Networks.  The basic approach will be:\n",
    "1. Load a categorical dataset (`diamond.csv`) \n",
    "3. Create 2 (related) Bayesian network models, fitting their parameters to the data.\n",
    "4. Compare the 2 structures, using log-likelihood.\n",
    "\n",
    "The data has 5 columns, with the first column being an index.  The first row gives the column names, $A, B, C, D$.  There are too many rows and columns to calculated this data by hand, so we will need to use computational tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as math\n",
    "\n",
    "dataframe = pd.read_csv('diamond.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Bayesian network structures\n",
    "We will look at two Bayesian network structures out of the very many that are possible for this dataset.\n",
    "2. Model $M_1$.\n",
    "$$ P_1(ABCD) = P(A)P(B|A)P(C|A)P(D|BC) $$\n",
    "2. Model $M_2$.\n",
    "$$ P_2(ABCD) = P(A)P(B|A)P(C|A)P(D|A) $$\n",
    "\n",
    "You will notice that the JPD is factorized, and that there are no graphs given. You can reconstruct the graph by reading the families.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diamond.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1.  Calculate the Conditional Probability tables for both models.\n",
    "In particular:\n",
    "* $P(A)$. (used in both models)\n",
    "* $P(B|A)$. (used in both models)\n",
    "* $P(C|A)$. (used in both models)\n",
    "* $P(D|BC)$. (used only in model $M_1$)\n",
    "* $P(D|A)$. (used only in model $M_2$)\n",
    "\n",
    "Calculate and display these tables neatly.  I recommend a Pandas dataframe for each table.  It will make step 2 easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPTs for Model One and Two\n",
      "P(A)\n",
      "A\n",
      "0    0.807385\n",
      "1    0.192615\n",
      "Name: A, dtype: float64\n",
      "\n",
      "P(B|A)\n",
      "B  A\n",
      "0  0    0.829630\n",
      "   1    0.206186\n",
      "1  0    0.170370\n",
      "   1    0.793814\n",
      "Name: B, dtype: float64\n",
      "\n",
      "P(C|A)\n",
      "C  A\n",
      "0  0    0.934568\n",
      "   1    0.597938\n",
      "1  0    0.065432\n",
      "   1    0.402062\n",
      "Name: C, dtype: float64\n",
      "\n",
      "CPTs for Model One only\n",
      "P(D|BC)\n",
      "B  C  D\n",
      "0  0  0    0.948092\n",
      "      1    0.051908\n",
      "   1  0    0.186441\n",
      "      1    0.813559\n",
      "1  0  0    0.309091\n",
      "      1    0.690909\n",
      "   1  0    0.216216\n",
      "      1    0.783784\n",
      "Name: D, dtype: float64\n",
      "\n",
      "CPTs for Model Two only\n",
      "P(D|A)\n",
      "D  A\n",
      "0  0    0.792593\n",
      "   1    0.371134\n",
      "1  0    0.207407\n",
      "   1    0.628866\n",
      "Name: D, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting CPTs from data using Pandas\n",
    "\n",
    "# for add-one smoothing\n",
    "smooth1D = np.array([1]*2)\n",
    "smooth2D = np.array([1]*4)\n",
    "smooth3D = np.array([1]*8)\n",
    "\n",
    "\n",
    "# P(A)\n",
    "# Adding smooth1D implements add-one smoothing, also known as Beta(1,1) prior\n",
    "count_A = smooth1D + dataframe.groupby(['A'])['A'].count()\n",
    "# a CPT can be construacted from a table of counts, if it is normalized\n",
    "P_A = count_A/count_A.sum()\n",
    "\n",
    "\n",
    "# P(B|A)\n",
    "count_BA = smooth2D + dataframe.groupby(['B','A'])['B'].count()\n",
    "# we sum along the 'B' axis to normalize\n",
    "denom_BA = count_BA.groupby(['A']).sum()\n",
    "P_B_A = count_BA/denom_BA\n",
    "\n",
    "\n",
    "# P(C|A)\n",
    "count_CA = smooth2D + dataframe.groupby(['C','A'])['C'].count()\n",
    "# we sum along the 'C' axis to normalize\n",
    "denom_CA = count_CA.groupby(['A']).sum()\n",
    "P_C_A = count_CA/denom_CA\n",
    "\n",
    "\n",
    "# P(D|BC) -- only in Model 1\n",
    "count_DBC = smooth3D + dataframe.groupby(['D','B','C'])['D'].count()\n",
    "# we sum along the 'D' axis to normalize\n",
    "denom_BC = count_DBC.groupby(['B','C']).sum()\n",
    "P_D_BC = count_DBC/denom_BC\n",
    "\n",
    "\n",
    "# P(D|A) -- only in Model 2\n",
    "count_DA = smooth2D + dataframe.groupby(['D','A'])['D'].count()\n",
    "# we sum along the 'D' axis to normalize\n",
    "denom_DA = count_DA.groupby(['A']).sum()\n",
    "P_D_A = count_DA/denom_DA\n",
    "\n",
    "\n",
    "###################################################\n",
    "print('CPTs for Model One and Two')\n",
    "print('P(A)')\n",
    "print(P_A)\n",
    "print()\n",
    "\n",
    "print('P(B|A)')\n",
    "print(P_B_A)\n",
    "print()\n",
    "\n",
    "print('P(C|A)')\n",
    "print(P_C_A)\n",
    "print()\n",
    "\n",
    "print('CPTs for Model One only')\n",
    "\n",
    "print('P(D|BC)')\n",
    "print(P_D_BC)\n",
    "print()\n",
    "\n",
    "print('CPTs for Model Two only')\n",
    "\n",
    "print('P(D|A)')\n",
    "print(P_D_A)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grading: 5 marks\n",
    "Your notebook calculates the 5 tables correctly, and presents them neatly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marking\n",
    "Counting is not the problem, but keeping the counts straight and normalized is a bit tricky.  I used Pandas dataframes, which I recommended, but figuring out how to do all the right summing is an exercise in skimming the Pandas documentation, and it's okay if it's done using Numpy or something else.\n",
    "\n",
    "The numbers themselves are not really the point either.  I used \"add one smoothing\" (also known as \"Beta(1,1) priors\") above, but that's not essential.  I just wanted to show it could be done.  It makes a small difference in the values, which is exactly what you'd expect.\n",
    "\n",
    "*  3 marks: Correctness\n",
    "      * The CPTs have to have the right number of values, e.g., P(A) has 2, P(B|A) has 4, etc\n",
    "      * The CPTs need to sum to 1 in the right ways.  \n",
    "      * There have to be 5 CPTs in total.  \n",
    "*  2 marks: Neatness\n",
    "      * The tables have at least minimal formatting; they can't simply be a list displayed to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2.  Compare the two models using the likelihood of the data\n",
    "Let $\\mathbf{X}$ represent the dataset.  We want to compare the following two quantities:\n",
    "* $P(\\mathbf{X} | M_1)$ that is, the likelihood of the data using $M_1$.\n",
    "* $P(\\mathbf{X} | M_2)$ that is, the likelihood of the data using the other model.\n",
    "\n",
    "Let $a,b,c,d$ be the values for the variables on one of the rows.  In the first model:\n",
    "$$ P(a,b,c,d|M_1) = P(a)P(b|a)P(c|a)P(d|bc) $$\n",
    "In other words, we are using the tables for $M_1$ in this factorization.  TO finish this calculation off, we have to look up one number from each table, and mutiply them all together.  These are the tables we calculated in Step 1.\n",
    "\n",
    "This is the likelihood of a single row of the data.  To calculate the likelihood of the whole data set, we assume each row is independent:\n",
    "$$ P(\\mathbf{X}|M_1) = \\prod_i P_1(a_i,b_i,c_i,d_i|M_1). $$\n",
    "In other words we multiply the likelihoods of all the rows together to get the likelihood of the dataset using Model $M_1$.\n",
    "\n",
    "Keep in mind this calculation is for the first model, $M_1$; a slightly different calculation is needed for the second model $M_2$, because the family for $D$ is different. \n",
    "\n",
    "When you have calculated the likelihood for the data in **both models**, we can say that the preferred model is the one where the likelihood is highest.\n",
    "\n",
    "Here's the task:  Compare the likelihood of the data in both models, and decide which one fits the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihoods:\n",
      "Model 1: -696.859\n",
      "Model 2: -787.921\n",
      "Difference: 91.062\n"
     ]
    }
   ],
   "source": [
    "# log likelihoods for the two models\n",
    "# because working in probability space will usually cause underflow\n",
    "\n",
    "loglik1 = 0\n",
    "loglik2 = 0\n",
    "\n",
    "# work through the data set, row by row\n",
    "for row in dataframe.itertuples():\n",
    "    # use the values on the row to index into the CPTs.\n",
    "    pa = P_A[row.A]\n",
    "    \n",
    "    # I guess Pandas was not really meant for this\n",
    "    # if you used a dictionary or a list, that's okay\n",
    "    pb = P_B_A.xs(row.B, level='B')[row.A]\n",
    "    pc = P_C_A.xs(row.C, level='C')[row.A]\n",
    "    pd1 = P_D_BC.xs(row.D, level='D').xs(row.B, level='B')[row.C]\n",
    "    pd2 = P_D_A.xs(row.D, level='D')[row.A]\n",
    "    \n",
    "    # now multiply probabilities (or add log probabilities)\n",
    "    common = pa * pb * pc\n",
    "    row_prob1 = common * pd1\n",
    "    row_prob2 = common * pd2\n",
    "    \n",
    "    # two accumulators for the two models\n",
    "    loglik1 += math.log(row_prob1, 10)\n",
    "    loglik2 += math.log(row_prob2, 10)\n",
    "    \n",
    "print('Log-likelihoods:')\n",
    "print('Model {}: {:.3f}'.format(1, loglik1))\n",
    "print('Model {}: {:.3f}'.format(2, loglik2))\n",
    "print('Difference: {:.3f}'.format(loglik1 - loglik2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood differences:\n",
      "Model 1: -142.675\n",
      "Model 2: -233.737\n",
      "Difference: 91.062\n"
     ]
    }
   ],
   "source": [
    "# Since parts of the two models are the same \n",
    "# we only need to keep track of what's different.\n",
    "\n",
    "# loglikelihood differences for the two models\n",
    "diff1 = 0\n",
    "diff2 = 0\n",
    "\n",
    "# work through the data set, row by row\n",
    "for row in dataframe.itertuples():\n",
    "    # only the model for Variable D is different\n",
    "    pd1 = P_D_BC.xs(row.D, level='D').xs(row.B, level='B')[row.C]\n",
    "    pd2 = P_D_A.xs(row.D, level='D')[row.A]\n",
    "\n",
    "    # just accumulate the differences\n",
    "    diff1 += math.log(pd1, 10)\n",
    "    diff2 += math.log(pd2, 10)\n",
    "    \n",
    "print('Log-likelihood differences:')\n",
    "print('Model {}: {:.3f}'.format(1, diff1))\n",
    "print('Model {}: {:.3f}'.format(2, diff2))\n",
    "print('Difference: {:.3f}'.format(diff1 - diff2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grading: 4 marks \n",
    "Your calculations are correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marking: \n",
    "A correct solution means that the method was largely correct, if not exactly as above.  \n",
    "I showed in the second version that the only part that needs to be calculated is the factor involving $D$, since the two models were exactly the same for the other variables. \n",
    "\n",
    "* 4 marks: Correctness.  To be correct, the script has to:\n",
    "    1. Loop through each row in the dataset\n",
    "    2. Calculate the probability of the values on the row, using both models.\n",
    "    3. Calculate the log-likelihood of the whole dataset with two models, because without the logs, underflow will occur. \n",
    "\n",
    "Deduct a mark for each item not done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "Which of the two model is the better fit for the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the log-likelihoods of the two models, Model 1 is better, because the log-likelihood is higher.  \n",
    "\n",
    "This is not too surprising given the name of the file.  Model 1 is a diamond shape, whereas Model 2 has the structure of Naive Bayes.  I generated synthetically, starting from a Bayesian network with the same diamond shape, with some artificially created CPTs, and randomly sampled values from the model.  It's a good thing that Model 1 is the better fit, because that's what we started with.  Had I started with Model 1, and generated data from it, it would have been the better model according to this analysis.\n",
    "\n",
    "I'm a little disappointed in the speed of the calculations.  There's probably a faster way to do these calculations with Pandas.  Also, most of my effort to complete this question was figuring out which Pandas methods to call.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grading: 1 mark \n",
    "Your conclusion is appropriate.\n",
    "\n",
    "#### Marking\n",
    "The correct conclusion is that Model 1 is the better one.  Do not give the mark if this conclusion is not the result of the effort.  There's a lot of generosus marking here, but this is where you have to get the right answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
