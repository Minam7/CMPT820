{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPT 423/820 Assignment 2 Question 4\n",
    "### Model Solution and Grading Scheme\n",
    "\n",
    "Currently, Scikit-Learn has 4 implementations of Naive Bayes.  Each implementation assumes that all the features have the same kind of feature distribution.  For example, the Scikit-Learn implementation of Gaussian Naive Bayes Classifier assumes that all features are numeric, and the histogram of the features given the class label are more-or-less bell shaped around a mean value.  On the other hand, the Scikit-Learn implementation of the Categorical Naive Bayes Classifier assumes that all features are categorical.\n",
    "**This is a limitation of the software, not a theoretical limitation of Naive Bayes.**\n",
    "\n",
    "In this question, you are invited to explain, or describe how we could use these two classifiers to handle mixed data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit Learn on mixed data\n",
    "Suppose we have a categorical class label $Y$, some continuous features $F$ and some categorical features $C$.  For our purposes, it doesn't matter how many of each we have.  I will derive a formula based on one of each, and generalize after that.\n",
    "\n",
    "The classification formula for Naive Bayes doesn't care about the distinction between continuous or categorical fetures, so we have the same formula as before:\n",
    "\n",
    "$$ y(c,f) = \\arg \\max_{Y} P(Y |c,f)  $$\n",
    "\n",
    "As usual, we will start with the class conditional distribution, and apply Bayes Rule, and conditional independence:\n",
    "\n",
    "$$ P(Y |c,f) = \\frac{P(c|Y)P(f|Y)P(Y)}{P(c,f)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Scikit Learn cannot fit this model with mixed data directly.  Not yet, anyway.  But It can fit continuous features using Gaussian Naive Bayes, and categorical features using Categorical Naive Bayes (in Version 0.22).  \n",
    "\n",
    "We can split the data into two parts, the categorical features, C (with Y), and the continuous features F (with Y).  Fitting them independently gives us 2 classifiers:\n",
    "\n",
    "$$ y_1(c) = \\arg \\max_{Y} P(Y |c)  = \\arg \\max_{Y} \\frac{P(c|Y)P(Y)}{P(c)} $$\n",
    "$$ y_2(f) = \\arg \\max_{Y} P(Y |f)  = \\arg \\max_{Y} \\frac{P(f|Y)P(Y)}{P(f)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is now, how to derive a formula for $y(c,f)$ given $y(c)$, and $y(f)$?\n",
    "\n",
    "Since $y(c,f)$ uses $P(c|Y)$ and $P(f|Y)$, I will find expressions for these two factors.  We start with \n",
    "\n",
    "$$ P(Y |c) = \\frac{P(c|Y)P(Y)}{P(c)} $$\n",
    "\n",
    "and rearrange to get\n",
    "\n",
    "$$ P(c|Y) = \\frac{P(Y|c)P(c)}{P(Y)} $$ \n",
    "\n",
    "(this is also just Bayes Rule).  Similarly:\n",
    "\n",
    "$$ P(f|Y) = \\frac{P(Y|f)P(f)}{P(Y)} $$ \n",
    "\n",
    "Now we substitute into $y(c,f)$ and simplify as follows:\n",
    "\n",
    "$$ P(Y |c,f) = \\frac{\\frac{P(Y|c)P(c)}{P(Y)}\\frac{P(Y|f)P(f)}{P(Y)}P(Y)}{P(c,f)}\n",
    "= \\frac{P(Y|c)P(Y|f)}{P(Y)} \\frac{P(c)P(f)}{P(c,f)} $$\n",
    "\n",
    "I've gathered the factors that depend on $Y$ and those that do not depend on $Y$.  Note that $P(c)P(f) \\not = P(c,f)$ in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally:\n",
    "\n",
    "$$ y(c,f) = \\arg \\max_{Y} P(Y |c,f)  \n",
    "=  \\arg \\max_{Y}  \\frac{P(Y|c)P(Y|f)}{P(Y)} \\frac{P(c)P(f)}{P(c,f)} \n",
    "=  \\arg \\max_{Y}  \\frac{P(Y|c)P(Y|f)}{P(Y)}  \n",
    "$$\n",
    "\n",
    "The last step is true because the factor involving $c,f$ does not depend on $Y$, so it's constant, and cannot affect the maximization.\n",
    "\n",
    "Scikit Learn allows us to access $P(X|Y)$ (where $X$ is any feature), and $P(Y)$ for any Naive Bayes classifier it fits using:\n",
    "* Any classifier\n",
    "    * Method `predict_proba()`: this is $P(X|Y)$ where $X$ is a feature.\n",
    "    * Method `predict_log_proba()`: this is $\\log P(X|Y)$ where $X$ is a feature.\n",
    "    * Attribute `class_prior_`: this is $P(Y)$\n",
    "* Categorical NB\n",
    "    * Attribute `feature_log_prob`: This is $\\log P(X|Y)$ where $X$ is a feature.\n",
    "    * Attribute `class_log_prior`: This is $\\log P(Y)$. \n",
    "* Gaussian NB\n",
    "    * Attribute `sigma_`, `theta_`: These are $\\mu_{FY}, \\sigma_{FY}$ for each feature $F$ and each class $Y$.  With these values, we can calculate $P(F|Y)$ using Numpy or Python's random modulem using the Normal distribution ${\\cal N}(\\mu_{FY}, \\sigma_{FY}^2)$\n",
    "\n",
    "Now we ask the two fitted classifiers for all the $P(C|Y)$ and $P(F|Y)$.  Then we can multiply these together (or better, add the log probabilities).   Then we get $P(Y)$ (from either classifier -- they should report the same values), and divide (or subtract the log priors).  Then it's a linear search through the probabilities, looking for the maximum.\n",
    "\n",
    "Here's Pythonesque pseudo-code, ignoring details like multi-dimensional arrays returned by Scikit Learn:\n",
    "```\n",
    "    # To fit the mixed classifier\n",
    "    clf_1 = CategoricalNB()\n",
    "    clf_2 = GaussianNB()\n",
    "    clf_1.fit(C,Y)\n",
    "    clf_1.fit(F,Y)\n",
    "\n",
    "    # to classify a given sample c,f:\n",
    "    lP_Y = clf_1.class_log_prior_\n",
    "    lP_C_Y = clf_1.feature_log_proba\n",
    "    mu, sigma = clf_2.mu_, clf_2.sigma\n",
    "    P_F_Y = NormalVariate(F, mu, sigma)\n",
    "    \n",
    "    lP_Y_CF = lP_C_Y + lP_F_Y + log(P_F_Y) - lP_Y\n",
    "    select from lP_Y_CF the largest value\n",
    "    return the corresponding Y value\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Guideline\n",
    "This question should be graded according to the level of accomplishment achieved.  It is an open-ended question.\n",
    "\n",
    "The derivation above is an example of a formal derivation, based on the probabilistic foundations of Naive Bayes.  \n",
    "* A good formal derivation (10/10) will:\n",
    "    * Apply the two simpler formulae\n",
    "    * Account for $P(Y)$ appearing in both of the smaller classifiers.\n",
    "    * Rule out the constant factor as irrelevant.\n",
    "    * Show that Scikit Learn can give us the information we need in the form of attributes or methods\n",
    "    \n",
    "* A weaker derivation (7/10) will \n",
    "    * Multiply the results of the method `predict_proba()` which is $P(X|Y)$ together.\n",
    "    * Ignore or omit $P(Y)$\n",
    "    * Try to calculate the constant factor\n",
    "    * Assume that the constant factor is equal to 1\n",
    "\n",
    "* An adequate informal description (6/10) can:\n",
    "    * suggest the use of `predict_proba()` for the question, without a proof or derivation\n",
    "    * suggest asking both classifiers for a class, and use the answer as a vote; for this a tie breaking scheme needs to be included.\n",
    "\n",
    "* A weak informal description (3/10) might:\n",
    "    * suggest asking both classifiers for a class, and use the answer as a vote without a tie breaking scheme.\n",
    "    * suggest some plausible approach not making use of the Naive Bayes classifiers provided by Scikit Learn.\n",
    "\n",
    "* A totally inadequate description (0/10):\n",
    "    * suggest an implausible approach\n",
    "    \n",
    "\n",
    "A demonstration requires:\n",
    "* Creating a mixed data set, or finding a mixed dataset on line. \n",
    "* Applying the technique above to the data set.\n",
    "* Evaluating performance by:\n",
    "    * Comparing accuracy (or error) of some examples\n",
    "    * Evaluation of accuracy (or error) on a test set\n",
    "\n",
    "\n",
    "### Grading Summary\n",
    "* Approach (10 possible marks)\n",
    "    * Nothing submitted: 0/10  \n",
    "    * Totally inadequate: 0/10\n",
    "    * Informal, but weak: 3/10\n",
    "    * Adequate Informal: 6/10\n",
    "    * Weak formal derivation: 7/10\n",
    "    * Good formal derivation: 10/10\n",
    "* Demonstration (5 possible marks)\n",
    "    * Nothing submitted: 0/5\n",
    "    * Showed a few examples: 2/5\n",
    "    * Fitted with mixed training set and evaluated with mixed test set: 5/5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
