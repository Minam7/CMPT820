{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPT 423/820 \n",
    "## Assignment 2 Question 4\n",
    "* Seyedeh Mina Mousavifar\n",
    "* 11279515\n",
    "* sem311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(w_j|\\mathbf{x}_i) = \\frac{P(\\mathbf{x}_i|w_j)P(w_j)}{P(\\mathbf{x}_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathbf{x}_i$ is the feature vector of sample $i$, $i \\in {1,2,...,n}$,\n",
    "\n",
    "$w_j$ is the notation of class $j$, $j \\in {1,2,...,m}$,\n",
    "\n",
    "$P(\\mathbf{x}_i|w_j)$ is the probability of observing sample $\\mathbf{x}_i$ given that it belongs to class $w_j$.\n",
    "\n",
    "\n",
    "So the decision rule is:\n",
    "\n",
    "$$\n",
    "predicted \\; class \\; label \\leftarrow arg \\; max \\; P(w_j|\\mathbf{x}_i) \\quad for \\; j=1,...,m\n",
    "$$\n",
    "\n",
    "Further, the class condition probilities of individual features $d$ are as follows because of *naive* conditional independence assumption.\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|w_j) = P(x_1|w_j)...P(x_d|w_j) = \\Pi_{k=1}^{d}P(x_k|w_j)\n",
    "$$\n",
    "\n",
    "Another assumption of *Naive Bayes* is that $P(x_i=b|w_j)$ is drawn from a particular distribution, which is the reason of calling *Naive Bayes* a *generative model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *Bernoulli distribution* to compute the likelihood of a binary variable. For example, we could estimate $P(x_k=1 | w_j)$ via *MLE* as the frequency of occurences in the training set:\n",
    "\n",
    "$$\n",
    "\\theta = P(x_k=1|w_j) = N_{x_k, w_j} / N_{w_j}\n",
    "$$\n",
    "\n",
    "This means, number of training samples in class $w_j$ that have the property $x_k=1$ noted by $N_{x_k, w_j}$ divided by by all training samples in ωj, noted by $N_{w_j}$.\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|w_j) = \\Pi_{k=1}^{d}P(x_k|w_j) \\quad (1.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|w_j) = \\Pi_{k=1}^{d}(\\theta^{x_k}(1-\\theta)^{1-x_k})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the *Gaussian Naive Bayes* model is used for variables on a continuous scale, assuming that the variables are normally distributed.\n",
    "\n",
    "$$\n",
    "P(x_k|w_j) = \\frac{1}{\\sqrt{2\\pi\\sigma_{w_j}^{2}}}exp(-\\frac{(x_k - \\mu_{w_j})^2}{2\\sigma_{w_j}^{2}})\n",
    "$$\n",
    "\n",
    "Therefore, we need to estimate mean $\\mu$ of the samples associated with class $w_j$ and the variance $\\sigma^2$ associated with class $w_j$.\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|w_j) = \\Pi_{k=1}^{d}P(x_k|w_j) \\quad (1.2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *Naive Bayes* assumes conditional independence between features, it can be written as:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|w_j) = \\Pi_{k=1}^{d}P(x_k|w_j)\n",
    "$$\n",
    "\n",
    "By assuming that features $a, ..., b$ are from *Bernoulli* distribution and the rest are from *Guassian* distribution, we can write the above equation as:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|w_j) = \\Pi_{k=a}^{b}P(x_k|w_j)\\Pi_{k=b+1}^{d}P(x_k|w_j)\n",
    "$$\n",
    "\n",
    "So considering equation $(1.1)$, and $(1.2)$ the equation would be:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|w_j) = P(\\mathbf{x}_{bernoulli}|w_j)P(\\mathbf{x}_{guassian}|w_j)\n",
    "$$\n",
    "\n",
    "This can also be applied to *Multinoulli* instead of *Bernoulli* distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, I'll use *Forest covertypes* dataset. \n",
    "\n",
    "In this dataset, there are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described [here](https://archive.ics.uci.edu/ml/datasets/Covertype). Some of the features are boolean indicators, while others are discrete or continuous measurements.\n",
    "\n",
    "It has 54 columns of data. 10 quantitative variables, 4 binary wilderness areas and 40 binary soil type variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _covtype_dataset:\n",
      "\n",
      "Forest covertypes\n",
      "-----------------\n",
      "\n",
      "The samples in this dataset correspond to 30×30m patches of forest in the US,\n",
      "collected for the task of predicting each patch's cover type,\n",
      "i.e. the dominant species of tree.\n",
      "There are seven covertypes, making this a multiclass classification problem.\n",
      "Each sample has 54 features, described on the\n",
      "`dataset's homepage <https://archive.ics.uci.edu/ml/datasets/Covertype>`__.\n",
      "Some of the features are boolean indicators,\n",
      "while others are discrete or continuous measurements.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ============\n",
      "    Classes                        7\n",
      "    Samples total             581012\n",
      "    Dimensionality                54\n",
      "    Features                     int\n",
      "    =================   ============\n",
      "\n",
      ":func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;\n",
      "it returns a dictionary-like object\n",
      "with the feature matrix in the ``data`` member\n",
      "and the target values in ``target``.\n",
      "The dataset will be downloaded from the web if necessary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "import pandas as pd\n",
    "\n",
    "covtype = fetch_covtype()\n",
    "\n",
    "data = pd.DataFrame.from_records(covtype.data)\n",
    "labels = covtype.target\n",
    "\n",
    "print(covtype.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set aside  data as a part of test set\n",
    "tpropn = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, \n",
    "                                                    labels, \n",
    "                                                    test_size=tpropn)\n",
    "\n",
    "\n",
    "# continues part of train data\n",
    "data_con = X_train.values[:, :10]\n",
    "test_con = X_test.values[:, :10]\n",
    "\n",
    "# discrete part of train data\n",
    "data_cat = X_train.values[:, 10:]\n",
    "test_cat = X_test.values[:, 10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I independently fit a *Gaussian NB model* on the continuous part of the data and a *Multinomial NB model* on the categorical part. Then transform all the dataset by taking the class assignment probabilities, with *predict_proba* method. Then I multiply these probabilities and find the most probability for each record, as predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# fitting model for categorical part\n",
    "clf_cat = CategoricalNB()\n",
    "clf_cat.fit(data_cat, Y_train)\n",
    "\n",
    "# fitting model for continuous part\n",
    "clf_con = GaussianNB()\n",
    "clf_con.fit(data_con, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_predictor(model_cat,\n",
    "                    model_con,\n",
    "                    data_in_cat,\n",
    "                    data_in_con):\n",
    "    \"\"\"\n",
    "    :purpose: This function find class labels with\n",
    "     Naive Bayes classifier on mixed class.\n",
    "    :param model_cat: learned categorical NB\n",
    "    :param model_cat: learned continuous NB\n",
    "    :param data_in_cat: categorical part of dataset\n",
    "    :param data_in_con: continues part of dataset\n",
    "    :return: labels on the dataset\n",
    "    \"\"\"\n",
    "    prob_cat = model_cat.predict_proba(data_in_cat)\n",
    "    prob_con = model_con.predict_proba(data_in_con)\n",
    "    new_feature = prob_cat*prob_con\n",
    "\n",
    "    # prediction based on probabilites\n",
    "    new_feature = pd.DataFrame(new_feature,\n",
    "                 columns=range(1,8))\n",
    "\n",
    "    new_feature['pred_label'] = new_feature.idxmax(axis=1)\n",
    "\n",
    "    return new_feature.pred_label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMulti-class NB classifier\u001b[0m\n",
      "type            F1-score       \n",
      "training set    0.4895575148518661\n",
      "test set        0.48803086931518586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# on training set\n",
    "y_predicted = mixed_predictor(clf_cat,\n",
    "                              clf_con,\n",
    "                              data_cat,\n",
    "                              data_con)\n",
    "f1 = f1_score(Y_train,\n",
    "              y_predicted,\n",
    "              average='macro')\n",
    "\n",
    "acc = accuracy_score(Y_train,\n",
    "              y_predicted)\n",
    "\n",
    "# printing result in tabular format\n",
    "print('\\033[1m' + 'Multi-class NB classifier' + '\\033[0m')\n",
    "print('{:<15} {:<15}'.format('type',\n",
    "                            'F1-score',\n",
    "                            'Accuracy'))\n",
    "\n",
    "print('{:<15} {:<15}'.format('training set',\n",
    "                             f1, \n",
    "                             acc))\n",
    "\n",
    "# on test set\n",
    "y_predicted = mixed_predictor(clf_cat,\n",
    "                              clf_con,\n",
    "                              test_cat,\n",
    "                              test_con)\n",
    "f1 = f1_score(Y_test,\n",
    "              y_predicted,\n",
    "              average='macro')\n",
    "\n",
    "acc = accuracy_score(Y_test,\n",
    "              y_predicted)\n",
    "\n",
    "print('{:<15} {:<15}'.format('test set',\n",
    "                             f1, \n",
    "                             acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
